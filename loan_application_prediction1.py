# -*- coding: utf-8 -*-
"""loan application prediction1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1H-3vejDOQpVRp-zj8uKLmvqdt3wPDkQQ
"""

from google.colab import drive
drive.mount('/content/loan_application_approval')

# Importing the pandas library for data manipulation and analysis
import pandas as pd

# Importing the numpy library for numerical computations
import numpy as np

# Importing LabelEncoder for encoding categorical labels into numerical values
# Importing StandardScaler for standardizing/normalizing numerical features
from sklearn.preprocessing import LabelEncoder, StandardScaler

# Importing train_test_split to split data into training and testing sets
from sklearn.model_selection import train_test_split

# Importing TensorFlow, an open-source machine learning library
import tensorflow as tf

# Importing Sequential, a linear stack of layers for building a neural network
from tensorflow.keras import Sequential

# Importing Dense, a fully connected layer used in neural networks
from tensorflow.keras.layers import Dense

# Importing Dropout, a regularization technique to prevent overfitting by randomly dropping neurons during training
from tensorflow.keras.layers import Dropout

#data viaualization libraries
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
file_path = "/content/loan_application_approval/MyDrive/loan_approval_ann/loan_approval (1).csv"
df = pd.read_csv(file_path)

# Display the first few rows of the dataset
(df.head())

df = pd.read_csv("/content/loan_application_approval/MyDrive/loan_approval_ann/loan_approval (1).csv", index_col=0)
(df.head())

df.info()

df.describe()

# Check for missing values
print("Missing values per column:\n", df.isnull().sum())

# Check for duplicate rows
duplicates = df.duplicated().sum()
print(f"Number of duplicate rows: {duplicates}")

# Drop duplicate rows if any
df.drop_duplicates(inplace=True)

# Check distribution of numerical features
numerical_features = df.select_dtypes(include=['int64', 'float64']).columns
plt.figure(figsize=(15, 8))
df[numerical_features].hist(bins=20, figsize=(15, 8), layout=(3, 3))
plt.suptitle("Distribution of Numerical Features")
plt.show()

# Categorical feature count plots
categorical_features = df.select_dtypes(include=['object']).columns
for col in categorical_features:
    plt.figure(figsize=(8, 4))
    sns.countplot(x=df[col], palette="coolwarm")
    plt.title(f"Count Plot of {col}")
    plt.xticks(rotation=45)
    plt.show()

# Outlier detection using boxplots
for col in numerical_features:
    plt.figure(figsize=(8, 4))
    sns.boxplot(x=df[col], palette="coolwarm")
    plt.title(f"Boxplot of {col}")
    plt.show()

# Detecting outliers using IQR method
Q1 = df[numerical_features].quantile(0.25)
Q3 = df[numerical_features].quantile(0.75)
IQR = Q3 - Q1
outliers = ((df[numerical_features] < (Q1 - 1.5 * IQR)) | (df[numerical_features] > (Q3 + 1.5 * IQR))).sum()
display(outliers[outliers > 0])

plt.figure(figsize=(10, 6))
sns.heatmap(df.select_dtypes(include=['number']).corr(), annot=True, cmap="coolwarm", fmt='.2f')
plt.title("Feature Correlation Heatmap")
plt.show()

# Remove leading/trailing spaces from column names
df.columns = df.columns.str.strip()

# Identify categorical columns
categorical_cols = df.select_dtypes(include=['object']).columns

# Apply One-Hot Encoding
df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)

(df)

# Showing 5 rows from the dataset
print(df.head())

print("\n")

#Splitting the data into training and testing sets
X = df.drop('loan_status_ Rejected', axis = 1)
y = df['loan_status_ Rejected']

xtrain, xtest, ytrain, ytest = train_test_split(X, y, test_size = 0.2, random_state = 0)

print("Features/independent variables")
print(X)

print("\n")

print("Target/dependent variable")
print(y)

#standardizing the data
scaler = StandardScaler()
xtrain = scaler.fit_transform(xtrain)
xtest = scaler.transform(xtest)

#Create the model
model = Sequential([
    Dense(32, activation = 'relu', input_shape = (xtrain.shape[1],)),
    Dropout(0.1),
    Dense(32, activation = 'relu'),
    Dropout(0.5),
    Dense(1, activation = 'sigmoid')
])

#Model Compliation
model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])
model.summary()

#Training the model
model.fit(xtrain, ytrain, epochs = 20, batch_size = 16, validation_data = (xtest, ytest))

# Model Results
loss, accuracy = model.evaluate(xtest, ytest)
print(f'Test loss: {loss:.4f}')
print(f'Test accuracy: {accuracy:.4f}')

from tensorflow.keras.models import load_model

# Save the model
model.save('/content/loan_approval_model.h5')

import numpy as np
import pandas as pd

# Define the correct feature order (excluding loan_status and loan_id)
feature_columns = [
    "no_of_dependents", "income_annum", "loan_amount", "loan_term",
    "cibil_score", "residential_assets_value", "commercial_assets_value",
    "luxury_assets_value", "bank_asset_value", "education_ Not Graduate",
    "self_employed_ Yes"
]

# Prepare new data without 'loan_id' and with correct encoding
new_data = np.array([[2, 9600000, 29900000, 12, 778, 2400000, 17600000, 22700000, 8000000, 0, 0]])

# Convert to DataFrame
new_data_df = pd.DataFrame(new_data, columns=feature_columns)

# Scale the new data using the same scaler
new_data_scaled = scaler.transform(new_data_df)

# Predict using the trained model
prediction = model.predict(new_data_scaled)

# Convert probability to class (0 or 1)
predicted_class = (prediction > 0.5).astype(int)

print(f'Prediction: {predicted_class[0][0]}')

if predicted_class == 1:
    print("Loan Approved ")
else:
    print("Loan Rejected ")

import numpy as np
import pandas as pd

# Define the correct feature order (excluding loan_status and loan_id)
feature_columns = [
    "no_of_dependents", "income_annum", "loan_amount", "loan_term",
    "cibil_score", "residential_assets_value", "commercial_assets_value",
    "luxury_assets_value", "bank_asset_value", "education_ Not Graduate",
    "self_employed_ Yes"
]

# Using the values that previously resulted in a prediction of 1 (Rejected)
new_data = np.array([[2, 4100000, 12200000, 8, 417, 2700000, 2200000, 8800000, 3300000, 1, 1]])

# Convert to DataFrame
new_data_df = pd.DataFrame(new_data, columns=feature_columns)

# Scale the new data using the same scaler
new_data_scaled = scaler.transform(new_data_df)

# Predict using the trained model
prediction = model.predict(new_data_scaled)

# Convert probability to class (0 or 1)
predicted_class = (prediction > 0.5).astype(int)

print(f'Prediction: {predicted_class[0][0]}')  # Expecting 1 (Rejected)

if predicted_class == 1:
    print("Loan Approved ")
else:
    print("Loan Rejected ")